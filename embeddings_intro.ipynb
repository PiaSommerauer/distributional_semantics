{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional semantic models: \n",
    "## Hands-on partice\n",
    "\n",
    "\n",
    "This notebook will guide you through some steps to work with and understand the basic princinples of distributional semantic models. \n",
    "\n",
    "For background reading, please refer to the slides in this repository. For a quick introduction to word embeddings, [Sebastian Ruder's blog](https://ruder.io/word-embeddings-1/index.html) is a great start. \n",
    "\n",
    "### Disclaimer\n",
    "While there are many different algorithms to create such models, this notebook does not go into the details of them. Rather, it is supposed to provide an intuition of what distributional models can represent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we will show you:\n",
    "\n",
    "* How to load a distributional model in python (and where to download distributional models from)\n",
    "* How to train your own Word2Vec model (simple version). We also include and a pointer for training your own distributional model with control over hyperparameters and the option to compare unstable vactors (hyperwords code)\n",
    "* How to get insights into the quality and content of the distributional word representations using:\n",
    "    \n",
    "    (a) Simple cosine similarity operations\n",
    "    \n",
    "    (c) Running standard evaluation\n",
    "    \n",
    "    (b) Clustering as a way of exploring the semantics in word embeddings\n",
    "    \n",
    "    \n",
    "    \n",
    "* How to run standard evaluations\n",
    "\n",
    "In addition, the notebook contains a small exercise for getting started on Dutch (or other language) data.\n",
    "    \n",
    "\n",
    "**About this notebook:**\n",
    "\n",
    "This notebook is using python 3.6. It is recommeded to run it using Anaconda (which includes most packages used here). \n",
    "\n",
    "To open a notebook, open your terminal and type 'jupyter notebook'. This will open your standard browser and show your directory tree. Navigate to the right directory (i.e. the location where you stored your notebook) and click on it to open it. You can run cells (i.e. execute code) by clicking the 'play' button on top or hitting shift + enter. \n",
    "\n",
    "Since embedding models simply consist of a big matrix, you use standard python libraries (e.g. numpy) to deal with them. To get a first impression, you can use Gensim, which already includes basic model operations. To find out more about it, check out the documentation [here](https://radimrehurek.com/gensim/models/keyedvectors.html). \n",
    "\n",
    "I recommed installing Gensim via pip or directly from this notebook using 'command line magic' (simply run the code cell below). \n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "In addition, this notebook uses the following libraries: \n",
    "\n",
    "* NLTK (Natural language processing toolkit)\n",
    "* Spacy (optional)\n",
    "* Numpy \n",
    "* Scikit learn\n",
    "\n",
    "Please install them via pip or the notebook in case you haven't done so already. (Remove the '#' to install the additional libraries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/piasommerauer/anaconda3/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /Users/piasommerauer/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#You can use pip install from within the notebook like this:\n",
    "\n",
    "#%pip install gensim\n",
    "#%pip install numpy\n",
    "#%pip install sklearn #(scikit learn)\n",
    "#%pip install spacy\n",
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading or creating a distributional semantic model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Links for downloading existing models\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages (even Dutch with a bit of luck). \n",
    "\n",
    "**Large and commonly used models (English)**:\n",
    "\n",
    "* Google word2vec: o be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamtersdimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "\n",
    "**Various models in English & other languages**:\n",
    "\n",
    "* word2vec trained on Wikipedia for various languages (including Dutch): https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "* Word2vec wikipedia for English and German: https://github.com/idio/wiki2vec\n",
    "\n",
    "* fastText for languages other than English: https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n",
    "\n",
    "[TO DO: turn into table with speifications: Language, underying corpus, underlying corpus size, algorithm, hyperparameters]\n",
    "\n",
    "\n",
    "Gensim even lets you download models directly via their api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Creating your own model - the quick out-of-the box way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required installations**\n",
    "\n",
    "* requests: used for downloading data - you can download data manually instead\n",
    "* Gensim: Word2vec implementation for python\n",
    "* An NLP package for preprocessing: The examples here use NLTK ([Natural language processing toolkit](http://www.nltk.org/install.html). Alternatively, you can use [SpaCy](https://spacy.io/usage/models). If this is your first time using nltk, please make sure to download the most important corpora and resources (included in 'nltk book') but running `nltk.download()` (after having imported nltk). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use pip install from within the notebook like this:\n",
    "\n",
    "# add/remove packages you (don't) need to install here\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading nltk (only run this is you haven't done this already)\n",
    "import nltk \n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Download a corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a text corpus using python:\n",
    "# You can also do this manually by following this link: http://www.gutenberg.org/cache/epub/730/pg730.txt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# project Gutenberg Oliver Twist as a .txt file:\n",
    "url = 'http://www.gutenberg.org/cache/epub/730/pg730.txt'\n",
    "r = requests.get(url)\n",
    "# Access content and decode bytes to utf-8\n",
    "text = r.content.decode('utf-8')\n",
    "\n",
    "\n",
    "# create directory for data\n",
    "if not os.path.isdir('../data/'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "# Write the text to a file and store it in our data directory (or do this step manually)\n",
    "with open('../data/oliver_twist.txt', 'w') as outfile:\n",
    "    outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Preprocess the text**\n",
    "    \n",
    " There are several choices you can make in the preprocessing step. In general, you want to remove everything from the data that may introduce artifacts. You can also consider further regularization steps, such as replacing all numeric characters by the same representation or using lower case spelling for the entire corpus. ATTENTION: There is a trade-off between generalization and information (e.g. Lower- and uppercase spelling can be a relevant distinction, consider apple vs Apple).\n",
    " \n",
    "For smaller corpora, you will most likely want to remove punctuation and perhaps include some more regularization. You can even consider lemmatizing the text. If you inspect larger models (e.g. Google word2vec), you will notice that the vocabulary contains punctiuation (and all kinds of other weird symbols). For such a large dataset, the noise introduced by these things can most likely be neglected. \n",
    "\n",
    " Here, we do the following:\n",
    " \n",
    " * remove punctuation\n",
    " * set everythin to lower case\n",
    " * cut the text in sentences, so it can be processed by the vanilla, out of the box word2vec implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'test', 'text'],\n",
       " ['let', \"'s\", 'see', 'if', 'this', 'works'],\n",
       " ['test']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    \n",
    "    # put to lower case \n",
    "    punct = string.punctuation \n",
    "    # Iterate over punctuation marks and replace them by an empty string one by one:\n",
    "    tokens_clean = []\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "            \n",
    "    return tokens_clean\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    clean_sentences = []\n",
    "    sentences = sent_tokenize(text.strip())\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = word_tokenize(s.lower())\n",
    "        tokens_clean = remove_punct(tokens)\n",
    "        clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences\n",
    "    \n",
    "test = \"This is a test text. Let's see if this works! TEST.\"\n",
    "\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', '_could_', 'the', 'boy', 'be', 'crying', 'for']]\n"
     ]
    }
   ],
   "source": [
    "# apply to your text corpus \n",
    "\n",
    "# load data:\n",
    "\n",
    "with open('../data/oliver_twist.txt') as infile:\n",
    "    text_oliver = infile.read()\n",
    "\n",
    "# clean: \n",
    "text_oliver_clean = preprocess(text_oliver)\n",
    "print(text_oliver_clean[201:202])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create word2vec model**\n",
    "\n",
    "Gensim allows you to train your own model. Here, we use a toy example, which should run on your local machine. If you'd like to train a larger corpus, you will most likely need to use a server.\n",
    "\n",
    "In the cell below, we train a model on the oliver twist novel you've downloaded in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# create directory for models\n",
    "if not os.path.isdir('../models/'):\n",
    "    os.mkdir('../models')\n",
    "\n",
    "oliver_w2v = Word2Vec(text_oliver_clean, size = 300, window = 4, min_count =2)\n",
    "# How to write out the model as a text file (vectors can be inspected easily)\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.txt')\n",
    "# How to write out the model as a binary file (cannot be inspected easily)\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we train a model on the movie reviews corpus included in nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "mr = Word2Vec(movie_reviews.sents())\n",
    "mr.wv.save_word2vec_format('../models/movies.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Advanced: Creating your own model with control over hyperparameters \n",
    "\n",
    "If you want to create your own embeddings, this repository is a good start: \n",
    "\n",
    "https://bitbucket.org/omerlevy/hyperwords/src/default/\n",
    "\n",
    "Attention: code requires python2. We're woring on a python 3 version. \n",
    "\n",
    "**This is optional and probably takes quite some.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing word representations of different models\n",
    "\n",
    "\n",
    "Models may be stored in different (and sometimes a little confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Even though there is existing software for inspecting and manipulating vectors (e.g. in the Gensim Word2vec toolkit), you can easily write code yourself using numpy (my preferred way of working). This way, you don't have to rely on non-transparent implementations (remember the analogy example...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Accessing models using the Word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is represented internally as a...\n",
      "<class 'gensim.models.word2vec.Word2Vec'>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-312c558c1f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moliver_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The model vocabulary is represented internally as a...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "# Explore the word2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(oliver_w2v))\n",
    "print()\n",
    "#####\n",
    "vocabulary = oliver_w2v.vocab\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(vocabulary.keys())[:20])\n",
    "print('Information stored in the vocabulary for a word:')\n",
    "print(vocabulary['man'])\n",
    "print()\n",
    "#####\n",
    "# To access the vector of a particular word, you can simply do the following:\n",
    "vec_word = oliver_w2v['day']\n",
    "# This way, you access the vector as a numpy array\n",
    "print('Representation of an individual word vector:')\n",
    "print(type(vec_word))\n",
    "print('Number of vector dimensions:', len(vec_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Accessing a model without a specific package\n",
    "\n",
    "It can be useful to know how you can load any model in python, even if it is not compatible with Gensim (...though most models are). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can write your own code for loading your model as a numpy matrix. \n",
    "# I suggest to do this\n",
    "import numpy as np\n",
    "\n",
    "def load_model(path):\n",
    "    \n",
    "    matrix = []\n",
    "    vocab = []\n",
    "    word2index_dict = dict()\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        lines = infile.read().split('\\n')\n",
    "        \n",
    "    for n, line in enumerate(lines[1:]):\n",
    "        line_list = line.split(' ')\n",
    "        word = line_list[0]\n",
    "        vocab.append(word)\n",
    "        vec = [float(v) for v in line_list[1:]]\n",
    "        matrix.append(vec)\n",
    "        word2index_dict[word] = n\n",
    "        \n",
    "    return np.array(matrix), vocab, word2index_dict\n",
    "        \n",
    "matrix, vocab, word2index_dict = load_model('../models/oliver.txt')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecting word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to explore what the model represents! Play around with similarity, nearest neighbors and analogies and try to get a feeling for what the vectors can do. Feel free to load existing models and compare what they represent. The code snippets below continue with the Oliver Twist toy example - so don't be disappointed if it returns nonsense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Simple vector operations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.82370281e-01,  5.93165934e-01,  4.96783644e-01, -3.98947954e-01,\n",
       "       -3.19281280e-01,  1.59066558e-01,  1.53194889e-01,  3.49473767e-02,\n",
       "       -2.44699180e-01, -4.65560630e-02,  1.31564081e-01,  6.49539977e-02,\n",
       "        1.64813548e-01,  2.79522926e-01, -1.17801167e-01, -3.68301660e-01,\n",
       "        4.70747352e-01, -2.62080431e-01, -1.01213902e-01, -5.20402670e-01,\n",
       "        5.03468513e-01, -4.83621247e-02,  2.48667568e-01, -2.72977799e-01,\n",
       "       -4.66402978e-01, -5.86646438e-01,  6.54029381e-03, -1.75723538e-01,\n",
       "       -7.20669568e-01, -1.86170533e-01,  2.93193907e-01, -2.22171158e-01,\n",
       "        4.56183612e-01, -6.34932876e-01, -1.00779049e-02, -9.12989117e-03,\n",
       "        1.29760295e-01,  1.10368304e-01, -1.09471932e-01,  3.66998971e-01,\n",
       "        5.73788285e-01,  4.98077385e-02,  3.06970656e-01,  1.19032852e-01,\n",
       "       -3.24137002e-01,  3.73061538e-01, -1.30913511e-01,  2.20018655e-01,\n",
       "       -1.79588199e-01, -6.00708881e-03,  2.31907576e-01, -3.57276678e-01,\n",
       "        2.81778902e-01,  1.06471367e-01, -6.69732839e-02, -4.35382454e-03,\n",
       "       -4.80092734e-01,  6.79350346e-02,  6.44799054e-01, -2.86519617e-01,\n",
       "       -2.27305934e-01, -1.68531284e-01,  5.47635555e-01,  2.55441636e-01,\n",
       "        2.39996389e-01,  1.32039219e-01, -1.47730887e-01, -1.95996404e-01,\n",
       "       -1.43104210e-01, -2.28628322e-01, -1.11806341e-01, -3.35741103e-01,\n",
       "       -5.57992458e-01,  7.01750740e-02, -2.72713881e-02,  6.05109155e-01,\n",
       "        1.33191154e-03, -1.54687256e-01, -8.78382847e-02,  4.38636899e-01,\n",
       "       -2.32692733e-01, -1.54204108e-02,  5.42676896e-02,  5.00984192e-02,\n",
       "        2.91389197e-01, -3.81570131e-01,  8.63032520e-01, -3.18803698e-01,\n",
       "        3.54736298e-01, -2.72872657e-01, -2.94130802e-01,  3.10124844e-01,\n",
       "       -3.88299167e-01, -2.11712241e-01, -4.93006408e-01, -2.94915568e-02,\n",
       "        2.28895128e-01, -1.86326340e-01, -3.21818627e-02, -2.08227843e-01,\n",
       "        5.15839100e-01,  1.72636926e-01, -3.66353571e-01,  4.98792410e-01,\n",
       "       -6.68685019e-01, -2.04860181e-01,  2.23736897e-01, -1.87485009e-01,\n",
       "        5.46677291e-01,  2.06290632e-01,  4.13641334e-01, -4.47237581e-01,\n",
       "       -3.66189837e-01, -3.97968322e-01,  2.14572862e-01, -1.38247296e-01,\n",
       "       -3.90331864e-01,  6.40317053e-02, -2.60347754e-01,  2.23305374e-01,\n",
       "        1.23932332e-01, -3.88389453e-02,  2.18381509e-01, -1.74844131e-01,\n",
       "       -4.73915398e-01, -2.91402370e-01,  2.04058960e-01,  2.46733025e-01,\n",
       "        2.12207902e-02,  3.11754405e-01,  4.40219700e-01, -3.99922669e-01,\n",
       "       -3.29314858e-01, -7.98907518e-01,  1.56640634e-02, -1.22832321e-01,\n",
       "        3.32155704e-01,  1.24380715e-01, -5.23330737e-03, -5.79547405e-01,\n",
       "        2.81928182e-01, -4.86186653e-01, -2.39285037e-01, -1.79646745e-01,\n",
       "        1.79610357e-01,  1.25427485e-01, -3.20690542e-01,  6.70352519e-01,\n",
       "       -2.62980819e-01, -5.80105424e-01,  3.05259943e-01,  2.28894696e-01,\n",
       "        8.81888380e-04, -2.70653188e-01, -1.12443656e-01, -1.77578911e-01,\n",
       "       -9.27014947e-02,  2.17972681e-01, -4.92279500e-01,  1.49187669e-01,\n",
       "       -1.44687831e-01,  1.59334783e-02, -1.10368825e-01,  2.23202750e-01,\n",
       "        2.57926852e-01, -4.84700114e-01, -4.81992096e-01, -1.45473555e-01,\n",
       "        2.51582146e-01,  3.48329276e-01, -2.34905019e-01, -8.74658525e-02,\n",
       "        1.32491767e-01,  1.25275910e-01,  2.03340828e-01,  2.84215391e-01,\n",
       "        2.70731896e-01, -1.62530765e-01, -1.81870162e-01,  3.04374129e-01,\n",
       "        6.11075640e-01, -1.49877086e-01, -1.86229631e-01,  3.99332911e-01,\n",
       "        2.70631254e-01, -6.66128993e-01,  2.29000598e-01, -2.97520280e-01,\n",
       "        3.23237807e-01,  1.87266350e-01, -6.04746640e-01,  2.33022049e-01,\n",
       "       -9.37342763e-01, -1.11015841e-01,  1.45760141e-02, -3.31861526e-01,\n",
       "        8.86181533e-01, -7.62167573e-02, -2.82608658e-01, -6.66322887e-01,\n",
       "        4.50253248e-01, -9.56065778e-04, -5.52836992e-02,  8.91488791e-01,\n",
       "       -1.00576058e-01, -1.01610348e-01,  3.43207903e-02,  2.79704928e-01,\n",
       "       -4.22461808e-01, -6.70147181e-01, -1.70760542e-01, -2.76889980e-01,\n",
       "        3.02534878e-01,  1.98099185e-02,  1.53353095e-01, -2.18319803e-01,\n",
       "        7.81370580e-01, -5.07678628e-01,  2.65311450e-01, -7.11955130e-02,\n",
       "        5.46323955e-01,  3.33999664e-01,  5.37866533e-01,  7.17441261e-01,\n",
       "       -3.87103140e-01, -9.60851252e-01,  3.13045830e-01,  1.05690658e-01,\n",
       "       -1.93037599e-01, -2.27552816e-01, -2.46858001e-01,  6.04183078e-01,\n",
       "        8.61026645e-02,  5.35773575e-01, -8.95697102e-02, -1.96053475e-01,\n",
       "       -2.51452744e-01, -1.20131209e-01,  3.83905888e-01,  6.58204675e-01,\n",
       "       -8.35244209e-02, -5.15112057e-02,  2.02097297e-01, -4.10879701e-01,\n",
       "       -3.31645310e-01,  5.17083168e-01, -1.51446447e-01, -2.65631992e-02,\n",
       "       -3.85587752e-01, -2.55238831e-01, -5.04755080e-01, -5.29362559e-01,\n",
       "        2.27213338e-01,  8.55618040e-04, -2.98015237e-01, -7.58938968e-01,\n",
       "        1.10508777e-01, -3.53062868e-01, -3.96881670e-01, -2.73087323e-01,\n",
       "       -5.56617379e-01, -6.29000723e-01, -4.94219482e-01,  3.03592950e-01,\n",
       "       -5.58117509e-01, -5.36080062e-01, -2.99230307e-01,  1.68640792e-01,\n",
       "        6.55800253e-02, -7.68769145e-01, -1.79970399e-01, -6.52011633e-02,\n",
       "       -2.92751074e-01,  3.52081120e-01,  5.76889336e-01,  4.11505014e-01,\n",
       "       -1.23601004e-01,  5.23759937e-03,  1.19820878e-01,  5.07574022e-01,\n",
       "       -7.87330344e-02,  3.07129413e-01, -1.55771405e-01, -1.34661421e-01,\n",
       "        4.84812945e-01,  1.44163132e-01,  1.55632928e-01,  2.79823303e-01,\n",
       "       -9.51403752e-02, -6.03231907e-01,  2.35251486e-02, -6.43621907e-02,\n",
       "       -5.95665686e-02,  1.49666891e-01, -4.31586429e-02,  2.62235962e-02,\n",
       "        2.39903312e-02, -8.54211077e-02,  7.20610321e-02,  5.09485900e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n",
    "\n",
    "oliver_w2v['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man and woman should be more similar than man and dog:\n",
      "True!\n",
      "man-woman 0.99055547\n",
      "man-dog 0.9834327\n"
     ]
    }
   ],
   "source": [
    "# similarity\n",
    "\n",
    "cos_man_woman = oliver_w2v.similarity('man', 'woman')\n",
    "cos_man_dog = oliver_w2v.similarity('man', 'dog')\n",
    "\n",
    "print(f'Man and woman should be more similar than man and dog:')\n",
    "if cos_man_woman > cos_man_dog:\n",
    "    print('True!')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)\n",
    "else:\n",
    "    print('False')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound 0.9998648762702942\n",
      "appearance 0.9998453855514526\n",
      "hastily 0.9998446702957153\n",
      "uttered 0.99983149766922\n",
      "housebreaker 0.9998262524604797\n",
      "apparently 0.9998228549957275\n",
      "pointed 0.9998195171356201\n",
      "boys 0.9998145699501038\n",
      "form 0.999801754951477\n",
      "resumed 0.9997976422309875\n"
     ]
    }
   ],
   "source": [
    "# nearest neighbors \n",
    "\n",
    "# Tip: use the help function if you want to explore the arguments\n",
    "#help(oliver_w2v.most_similar)\n",
    "nearest_neighbors = oliver_w2v.most_similar('dog', topn=10)\n",
    "for w, cos in nearest_neighbors:\n",
    "    print(w, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready 0.9981622695922852\n",
      "trouble 0.9981571435928345\n",
      "none 0.9981471300125122\n",
      "committed 0.9981460571289062\n",
      "lie 0.9981348514556885\n",
      "comes 0.9981276392936707\n",
      "whatever 0.9981229305267334\n",
      "appear 0.9981118440628052\n",
      "till 0.9980810880661011\n",
      "delicate 0.9980794191360474\n"
     ]
    }
   ],
   "source": [
    "# Analogy\n",
    "\n",
    "closest_to_predicted_vec = oliver_w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "\n",
    "for word, cosine in closest_to_predicted_vec:\n",
    "    print(word, cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy (generally applicable)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is one of the most important concepts to understand if you want to work with vectors. It is calculated as shown below:\n",
    "\n",
    "![Cosine similarity](images/cosine.png \"Logo Title Text 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "# Using numpy:\n",
    "\n",
    "def normalize_vector(vec):\n",
    "   \n",
    "    # magnitude of the vector\n",
    "    mag = math.sqrt(sum([pow(value, 2) for value in vec]))\n",
    "\n",
    "    unit_vec = []\n",
    "\n",
    "    for value in vec:\n",
    "        unit_vec.append(value/mag)\n",
    "    unit_vec = np.array(unit_vec)\n",
    "    return unit_vec\n",
    "    \n",
    "def get_cosine(vec1, vec2):\n",
    "\n",
    "    vec1_norm = normalize_vector(vec1)\n",
    "    vec2_norm = normalize_vector(vec2)\n",
    "\n",
    "    cos = np.dot(vec1_norm, vec2_norm)\n",
    "\n",
    "    return cos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b  Standard evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations sets can be found here:\n",
    "\n",
    "Similarity\n",
    "\n",
    "* WordSim 353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\n",
    "* SimLex 999: https://fh295.github.io/simlex.html\n",
    "* MEN: https://staff.fnwi.uva.nl/e.bruni/MEN\n",
    "* Luong rare words: http://www.bigdatalab.ac.cn/benchmark/bm/dd?data=Rare%20Word\n",
    "\n",
    "\n",
    "\n",
    "Analogy \n",
    "\n",
    "* Google test sets (combined): http://download.tensorflow.org/data/questions-words.txt\n",
    "* Google test sets (semantic and morphological)https://bitbucket.org/omerlevy/hyperwords/src/default/testsets/analogy/\n",
    "* BATS: http://vecto.space/projects/BATS/\n",
    "\n",
    "\n",
    "Gensim already contains functions to run some standard evaluations. ATTENTION: If you already have a Gensim version installed, make sure to update it. (Mine was out of date and the evaluations did not run.) \n",
    "\n",
    "**Discussion question: How can we compare the scores of the similarity and relatedness evaluations? How would you test whether the correlation between the model and human judgments of one model is better than the correlation between model and human judgments of another model?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson score (0.13150965540322956, 0.24490907660339356)\n",
      "Spearman Rho score SpearmanrResult(correlation=0.14072884589326437, pvalue=0.213089922097479)\n",
      "Proportion of out ov vocabulary words 77.33711048158641\n"
     ]
    }
   ],
   "source": [
    "# Gensim has the evaluation methods built in\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# no access to actual pairs\n",
    "# if you want to read up on the details, run:\n",
    "#help(oliver_w2v.evaluate_word_pairs)\n",
    "pearson, spearman, oov = oliver_w2v.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "\n",
    "print('Pearson score', pearson)\n",
    "print('Spearman Rho score', spearman)\n",
    "print('Proportion of out ov vocabulary words', oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogy evaluation = actually gives you the model output sorted into correct and \n",
    "# incorrect predictions \n",
    "score, output= oliver_w2v.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c Clustering \n",
    "\n",
    "\n",
    "A nice way of inspecting word vectors is testing how they behave in clustering. Scikit learn offers a number of implementations of different [clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html). \n",
    "\n",
    "\n",
    "Note: This is just to get a first impression. I recommed reading up on clustering evaluation for using larger sets without label annotations. Scikit learn is a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['apple', 'orange', 'lemon', 'strawberry', 'tomato']\n",
    "vegetables = ['cucumber', 'pepper', 'carrot', 'zucchini', 'egg_plant']\n",
    "animals = ['cat', 'dog', 'chicken', 'shrimp', 'lion', 'hamster', 'jaguar']\n",
    "abstract_concepts = ['feeling', 'idea', 'thought', 'theory', 'anger', 'aggression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat animal\n",
      "dog animal\n",
      "chicken animal\n",
      "shrimp animal\n",
      "lion animal\n",
      "hamster animal\n",
      "jaguar animal\n",
      "apple fruit\n",
      "orange fruit\n",
      "lemon fruit\n",
      "strawberry fruit\n",
      "tomato fruit\n",
      "cucumber vegetable\n",
      "pepper vegetable\n",
      "carrot vegetable\n",
      "zucchini vegetable\n",
      "egg_plant vegetable\n",
      "feeling abstract\n",
      "idea abstract\n",
      "thought abstract\n",
      "theory abstract\n",
      "anger abstract\n",
      "aggression abstract\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def map_words(words, label, word_label_dict):\n",
    "    for word in words:\n",
    "        word_label_dict[word] = label\n",
    "\n",
    "word_label_dict = dict()\n",
    "map_words(animals, 'animal', word_label_dict)\n",
    "map_words(fruits, 'fruit', word_label_dict)\n",
    "map_words(vegetables, 'vegetable', word_label_dict)\n",
    "map_words(abstract_concepts, 'abstract', word_label_dict)\n",
    "\n",
    "for label, words in word_label_dict.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken oov\n",
      "shrimp oov\n",
      "lion oov\n",
      "hamster oov\n",
      "jaguar oov\n",
      "apple oov\n",
      "lemon oov\n",
      "strawberry oov\n",
      "tomato oov\n",
      "cucumber oov\n",
      "pepper oov\n",
      "carrot oov\n",
      "zucchini oov\n",
      "egg_plant oov\n",
      "aggression oov\n"
     ]
    }
   ],
   "source": [
    "def get_all_vectors(word_label_dict, model):\n",
    "    \n",
    "    vecs = []\n",
    "    words_in_vocab = []\n",
    "    \n",
    "    for word in word_label_dict.keys():\n",
    "        if word in model.vocab:\n",
    "            vec = model[word]\n",
    "            vecs.append(vec)\n",
    "            words_in_vocab.append(word)\n",
    "        else:\n",
    "            print(word, 'oov')\n",
    "    \n",
    "    return np.array(vecs), words_in_vocab\n",
    "\n",
    "\n",
    "vecs, words_in_vocab = get_all_vectors(word_label_dict, oliver_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters 3\n"
     ]
    }
   ],
   "source": [
    "# Clustering doc: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "# as many clusters as classes:\n",
    "\n",
    "n_clusters = len(set(word_label_dict[word] for word in words_in_vocab))\n",
    "print('number of clusters', n_clusters)\n",
    "# abstract vs concrete?\n",
    "#n_clusters = 2\n",
    "y_pred = KMeans(n_clusters=n_clusters, init='random').fit_predict(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 2 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['cat', 'orange', 'idea', 'theory', 'anger']\n",
      "0 ['dog', 'thought']\n",
      "2 ['feeling']\n"
     ]
    }
   ],
   "source": [
    "predicted_clusters = defaultdict(list)\n",
    "for word, pred_label in zip(words_in_vocab, y_pred):\n",
    "    predicted_clusters[pred_label].append(word)\n",
    "    \n",
    "for label, words in predicted_clusters.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n"
     ]
    }
   ],
   "source": [
    "def normalize_vector(vec):\n",
    "   \n",
    "    # magnitude of the vector\n",
    "    mag = math.sqrt(sum([pow(value, 2) for value in vec]))\n",
    "\n",
    "    unit_vec = []\n",
    "\n",
    "    for value in vec:\n",
    "        unit_vec.append(value/mag)\n",
    "    unit_vec = np.array(unit_vec)\n",
    "    \n",
    "\n",
    "a_matrix = np.array([[1, 2], [3, 4]])\n",
    "centroid = np.mean(a_matrix, axis=0)\n",
    "print(centroid)\n",
    "\n",
    "\n",
    "\n",
    "# Steps:\n",
    "\n",
    "# Normalize each vector\n",
    "# Create list of all normalized vectors, then turn into np.array\n",
    "# get mean using axis = 0\n",
    "# get cosines between centroid and words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise for Dutch\n",
    "\n",
    "Spacy for Dutch: https://spacy.io/models/nl\n",
    "\n",
    "Spacy quickstart: https://spacy.io/usage\n",
    "\n",
    "1) Find a Dutch corpus\n",
    "\n",
    "2) Preprocess it using Spacy for Dutch\n",
    "\n",
    "3) Create a model (using Gensim or something else)\n",
    "\n",
    "4) See if you can get an impression of what it captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the Dutch models, please run the following line from the terminal. Make sure that the command `python` is linked to the same python version used by anaconda. You can use `python --version` to find out. \n",
    "\n",
    "`python -m spacy download nl_core_news_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "doc = nlp(u\"Dit is een zin.\")\n",
    "# Accessing tokens:\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
